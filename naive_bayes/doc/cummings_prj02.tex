\input{functions.tex}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}
\small

\title{Project 02}
\author{Evan Cummings\\
CSCI 544 - Machine Learning}

\maketitle

First, Bays theorem states that

$$ P(v_j | a_1, a_2, \ldots, a_n) = \frac{P(a_1, a_2, \ldots, a_n | v_j) P(v_j)}{P(a_1,a_2,\ldots,a_n)},$$

where $a$ is the attribute, $v$ is a value, $j$ is the number of different values and $n$ is the number of attributes.  If we assume condtional independence of the attribute values given the target value, we can say that $P(a_1, a_2, \ldots, a_n | v_j) = \prod_i P(a_i | v_j)$, where $i = 1,\ldots,n$.  Using this assumption, we can more easily estimate the class for which element $v_j$ belongs via the Naive Bayes classifier:

\begin{align}
  v_{NB} = \argmax_{v_j \in V}\left\{ P(v_j) \prod_i P(a_i | v_j) \right\},
\end{align}

where $v_{NB}$ denotes the target value output by the naive Bayes classifier.  The probability of picking value $j$ from the total pool of $m$ observations is given by

$$ P(v_j) = \frac{\#(v_j)}{m},$$

where $\#(v_j)$ is the number of elements with value $j$.  This value effectively provides a weighting to equation (1) by accounting for the percentage of 

\begin{multicols}{2}
\begin{figure}[H]
  \centering
		\includegraphics[width=0.45\textwidth]{images/normed.png}
\end{figure}
\vspace{5mm}

\begin{figure}[H]
  \centering
		\includegraphics[width=0.45\textwidth]{images/not_normed.png}
\end{figure}
\vspace{5mm}
\end{multicols}


\end{document}


